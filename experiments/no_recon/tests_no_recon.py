# This script is for matching an N-dimensional normal distribution to a 2D plane dataset. (N=latent_dim)import osimport torchimport torch.nn as nnimport torch.optim as optimfrom dmatch.models.nn.splines import StackedNonInvertibleNSFfrom torch.nn.utils import clip_grad_value_from tqdm import trangeimport numpy as npimport matplotlib.pyplot as pltfrom dmatch.utils.post_process import get_ood, get_salience, get_samples_modelfrom dmatch.models.nn.dense_nets import MLP, StochasticMLP, SplineNet, SkipNet, \    NsfOTfrom dmatch.utils import hyperparams, torch_utilsfrom dmatch.utils.MC_estimators import get_kl_and_errorfrom dmatch.utils.hyperparams import get_measure, get_distfrom dmatch.utils.io import get_top_dir, on_clusterfrom dmatch.utils.plotting import plot2Dhist, getCrossFeaturePlot, plot_spline, plot_coloured, plot_density, \    plot_salienceimport picklefrom dmatch.utils.training import get_log_detimport argparse#### Define arguments to pass from command line ######################################################################### from utils.test_stats import multivariate_normality# from utils.training import get_vars, get_grads, step_optimizersparser = argparse.ArgumentParser()## Savingparser.add_argument('-d', '--outputdir', type=str, default='no_recon',                    help='Choose the base output directory')parser.add_argument('-n', '--outputname', type=str, default='MPL',                    help='Set the output name directory')parser.add_argument('--plt', type=int, default=1, help='Integer whether to plot training and distributions.')parser.add_argument('--get_kl', type=int, default=0, help='Integer whether to calculate the KL divergence or not.')parser.add_argument('--get_sinkhorn', type=int, default=0, help='Integer whether to calculate the relative sinkorn'                                                                'distance or not.')parser.add_argument('--test_norm', type=int, default=0, help='Apply multivariate normal test.')parser.add_argument('--final_plot_encoding', type=int, default=0, help='Generate and save samples?')## Loadingparser.add_argument('--load', type=int, default=0, help='Whether to load existing experiment.')parser.add_argument('--retrain', type=int, default=0, help='Whether to load and retrain existing experiment.')## Base distribution argumentsparser.add_argument('--base_dist', type=str, default='shells',                    help='A string to index the corresponding torch distribution.')parser.add_argument('--std', type=float, default=0.01,                    help='Standard deviation defined in corresponding dataset (only for stars at present).')parser.add_argument('--latent_dim', type=int, default=20,                    help='The dimension of the base distribution.')parser.add_argument('--noise_strength', type=float, default=0,                    help='The number to multiply the standard normal that is added to the sample data by.')## Datasetparser.add_argument('--nsteps_train', type=int, default=100,                    help='The number of batches per epoch to train on, this with the batch size defines the size of '                         'the dataset.')parser.add_argument('--nsteps_val', type=int, default=10,                    help='The number of batches per epoch to use for validation.')parser.add_argument('--target', type=str, default='normal',                    help='A string to index the corresponding plane dataset distribution.')parser.add_argument('--target_dim', type=int, default=2,                    help='The dimension of the output.')parser.add_argument('--chain_dims', type=int, default=0,                    help='Set the latent dimension equal to the input dimension.')parser.add_argument('--normalise', type=int, default=1,                    help='Normalise the data?')## Model parametersparser.add_argument('--model', type=str, default='splash',                    help='The type of model to sue during training.')parser.add_argument('--train_ae', type=int, default=10,                    help='The type of model to sue during training.')parser.add_argument('--sp_init', type=str, default='relu',                    help='The type of initialization to apply to the spline.')parser.add_argument('--spline_type', type=str, default='SPLASH',                    help='The type of spline to use.')parser.add_argument('--nknots', type=int, default=30,                    help='The number of knots to use.')parser.add_argument('--nsplines', type=int, default=2,                    help='The number of knots to use.')parser.add_argument('--final_only', type=int, default=0,                    help='Apply the spline layer to the final activation only.')parser.add_argument('--dropout', type=float, default=0,                    help='A global parameter for controlling the dropout between layers.')parser.add_argument('--activation', type=str, default='none',                    help='The activation function to apply to the output of the "encoder".')parser.add_argument('--inner_activ', type=str, default='selu',                    help='The activation function to apply between layers of the "encoder".')parser.add_argument('--bnorm', type=int, default=0,                    help='An integer specifying whether to apply batch normalization to the model.')parser.add_argument('--lnorm', type=int, default=0,                    help='An integer specifying whether to apply layer normalization to the model.')parser.add_argument('--inorm', type=int, default=0,                    help='An integer specifying whether to apply instance normalization to the model.')parser.add_argument('--depth', type=str, default='paper2',                    help='A string defining whether the model should be deep or wide.')parser.add_argument('--stochastic', type=int, default=0,                    help='An integer specifying whether to make the model stochastic.')parser.add_argument('--weight_noise', type=float, default=0.01,                    help='The fraction of noise to apply to the weights of the loaded NSF.')parser.add_argument('--bias', type=int, default=1,                    help='Whether to use bias or not.')parser.add_argument('--sw', type=int, default=512, help='Set number of nodes.')parser.add_argument('--sd', type=int, default=3, help='Set depth.')parser.add_argument('--resnet_depth', type=int, default=2, help='Set depth.')## Training parametersparser.add_argument('--batch_size', type=int, default=1000, help='Size of batch for training.')parser.add_argument('--epochs', type=int, default=10, help='The number of epochs to train for.')parser.add_argument('--optim', type=str, default='Adam', help='The optimizer to use for training.')parser.add_argument('--lr', type=float, default=0.0001, help='The learning rate.')parser.add_argument('--wd', type=float, default=0.001,                    help='The weight decay parameter to use in the AdamW optimizer.')parser.add_argument('--p', type=float, default=0.9,                    help='The momentum to use in SGD.')parser.add_argument('--gclip', type=float, default=5,                    help='Gradient clipping.')parser.add_argument('--scheduler', type=str, default=None,                    help='Type of scheduler to use, "plateau" or "cosine".')parser.add_argument('--reset_optimizer', type=int, default=0,                    help='The number of epochs to train for.')parser.add_argument('--reset_lr', type=float, default=0.001,                    help='The number of epochs to train for.')parser.add_argument('--reset_wd', type=float, default=0.001,                    help='The number of epochs to train for.')parser.add_argument('--reset_p', type=float, default=0.9,                    help='The number of epochs to train for.')parser.add_argument('--beta_j', type=float, default=0,                    help='The number to multiply the log determinant in the loss by.')parser.add_argument('--beta_dist', type=float, default=0,                    help='The number to multiply the distribution matching in the AE by.')parser.add_argument('--det_only', type=int, default=0,                    help='The number to multiply the log determinant in the loss by.')parser.add_argument('--auto_J', type=int, default=0,                    help='Calculate the determinant using autograd?')parser.add_argument('--true_likelihood', type=int, default=0,                    help='Include the exact likelihood?')## Distribution measureparser.add_argument('--dist_measure', type=str, default='mmd',                    help='The type of distribution matching to use.')## KL estimateparser.add_argument('--nrun', type=int, default=2,                    help='The number of MC KL estimates to calculate.')parser.add_argument('--ncalc', type=int, default=int(1e5),                    help='The number of samples to pass through the encoder per sample.')parser.add_argument('--n_test', type=int, default=10,                    help='The number of times to calculate ncalc samples.')## Reproducibilityparser.add_argument('--multi_jobs', type=int, default=0,                    help='The number of times to calculate ncalc samples.')#### Collect arguments and begin script ################################################################################args = parser.parse_args()# Set seeds for reproducibility, if gathering stats this will be changed for each runif args.multi_jobs > 0:    seed_add = int(args.outputname[-1]) + args.multi_jobselse:    seed_add = 0torch.manual_seed(42 + seed_add)np.random.seed(42 + seed_add)output_dim = args.target_dimif args.chain_dims:    latent_dim = output_dim    args.latent_dim = output_dimelse:    latent_dim = args.latent_dimif args.reset_optimizer == 0:    args.reset_optimizer = args.epochsexp_name = args.outputname + '_' + str(latent_dim)top_dir = get_top_dir()sv_dir = top_dir + '/images' + '/' + args.outputdir + '/'if not os.path.exists(sv_dir):    os.makedirs(sv_dir, exist_ok=True)ld = args.loadretrain = args.retrainget_kl = args.get_kl# Load an experiment or launch a new oneif ld:    class Bunch(object):        def __init__(self, adict):            self.__dict__.update(adict)    # with open(sv_dir + 'exp_info_{}.pkl.png'.format(exp_name), 'rb') as f:    with open(sv_dir + 'exp_info_{}.pkl'.format(exp_name), 'rb') as f:        args = Bunch(pickle.load(f))else:    with open(sv_dir + 'exp_info_{}.pkl'.format(exp_name), 'wb') as f:        pickle.dump(vars(args), f)### Define the distributionsbase_dist = get_dist(args.base_dist, latent_dim, std=args.std)target_dist = get_dist(args.target, output_dim)### Get the devicedevice = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")prior = get_dist(args.target, output_dim, device=device)print(device)### Define the distribution matching termdist_measure = get_measure(args.dist_measure)### Define the model to trainlayers_outer = [args.sw] * args.sdout_activ = hyperparams.activations[args.activation]int_activ = hyperparams.activations[args.inner_activ]drop_p = args.dropoutif args.stochastic:    model = StochasticMLP(latent_dim, output_dim, layers=layers_outer, output_activ=out_activ, drp=drop_p,                          batch_norm=args.bnorm, layer_norm=args.lnorm, vae=False).to(device)else:    if args.model == 'MLP':        model = MLP(latent_dim, output_dim, layers=layers_outer, output_activ=out_activ, drp=drop_p,                    batch_norm=args.bnorm, bias=args.bias,                    layer_norm=args.lnorm, int_activ=int_activ,                    inst_norm=args.inorm).to(device)    if args.model == 'splash':        model = SplineNet(latent_dim, output_dim, layers=[128, 128, 128], output_activ=out_activ,                          init='relu', nsplines=7, spline_type='splash',                          final_only=False, nstop=4).to(device)    if args.model == 'nsf':        if latent_dim != output_dim:            raise Exception('The NSF only works on dimension preserving maps')        if args.base_dist == 'normal':            tails = 'linear'        else:            tails = None        model = NsfOT(latent_dim, nsplines=args.nknots, tails=tails, spline=1).to(device)    if args.model == 'ni_nsf':        if latent_dim != output_dim:            raise Exception('The ni-NSF only works on dimension preserving maps')        nstack = 4        tail_bound = 4.        if args.base_dist == 'normal':            tails = 'linear'        else:            tails = None        model = StackedNonInvertibleNSF(latent_dim, 128, num_bins=args.nknots, tail_bound=tail_bound, tails=tails,                                        nstack=nstack).to(device)    if args.model == 'affine':        if latent_dim != output_dim:            raise Exception('Flows only works on dimension preserving maps')        model = NsfOT(latent_dim, spline=0).to(device)    if args.model == 'skip_net':        model = SkipNet(latent_dim, output_dim, output_activ=out_activ, num_blocks=args.sd, skip_dim=args.sw,                        resnet_depth=args.resnet_depth).to(device)    elif args.model == 'trained_spline':        # Load a pretrained NSF and train further with OT losses        from nflows import flows        from dmatch.models.flow_models import flow_builder        from dmatch.models.nn.flows import autoregessive        transformation = autoregessive(2, 128, num_blocks=2, nstack=4,                                       tail_bound=4.0,                                       tails=None, activation=hyperparams.activations['leaky_relu'], num_bins=30,                                       spline=1)        base_dist_flow = hyperparams.nflows_dists('normal', 2, shift=None, bound=4.0)        flow = flows.Flow(transformation, base_dist_flow)        flow_model = flow_builder(flow, base_dist_flow, device, exp_name, directory='dummy')        flow_model.load(get_top_dir() + f'/experiments/data/saved_models/model_flows_paper_{args.base_dist}',                        device=device)        # Add some small amount of noise to the weights        weight_noise = args.weight_noise        with torch.no_grad():            for param in flow_model.parameters():                param.add_(torch.randn(param.size()) * weight_noise)        model = flow_model.to(device)if args.train_ae:    decoder = MLP(output_dim, latent_dim, layers=layers_outer, output_activ=nn.Identity()).to(device)    # decoder = dense_decoder(output_dim, latent_dim, layers=layers_outer, output_activ=nn.Identity()).to(device)else:    decoder = Nonenparams = sum(p.numel() for p in model.parameters() if p.requires_grad)print(f'There are {nparams} parameters in the model.')### Training definitionsbatch_size = args.batch_sizensteps_train = args.nsteps_trainnsteps_val = args.nsteps_valn_epochs = args.epochs### Optimizer and schedulerdef get_optimizer_schedulers(model, n_epochs, lr, wd, p, decoder=None):    params = list(model.parameters())    if decoder is not None:        params += list(decoder.parameters())    optimizer = hyperparams.get_optimizer(args.optim, params, lr=lr, wd=wd, momentum=p)    max_step = nsteps_train * n_epochs    if args.scheduler == 'cosine':        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, max_step, 0)    else:        scheduler = None    reduce_lr_outer = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')    return optimizer, scheduler, reduce_lr_outern_epochs_stage_one = args.train_ae if args.train_ae > 0 else n_epochsoptimizer, scheduler, reduce_lr_outer = get_optimizer_schedulers(model, n_epochs_stage_one, args.lr, args.wd, args.p,                                                                 decoder=decoder)### Train the modelmonitor_interval = 100tbar = trange(n_epochs, position=0)train_save = []val_save = []class data_handler():    def __init__(self, batch_size, nsteps_train, nsteps_val=0, normalise=False):        self.nsample = batch_size * nsteps_train        self.nval = batch_size * nsteps_val        self.batch_size = batch_size        self.nsteps_train = nsteps_train        self.nsteps_val = nsteps_val        self.max_val = None        self.min_val = None        self.normalise = normalise        self.update_data()    def manipulate(self, data):        data_len = data.shape[0]        # This is necessary because I run a slim version of hepmass locally        if data_len >= (self.batch_size * latent_dim):            # Truncate data that won't fit into a batch, this isn't problematic with generators            data = data[:data_len - (data_len % (self.batch_size * latent_dim))]            data = data.view(-1, self.batch_size, latent_dim)        else:            data = data.view(-1, *data.shape)        return data + torch.randn_like(data) * np.sqrt(args.noise_strength)    def update_data(self, valid=False):        if valid:            n_sample = self.nval        else:            n_sample = self.nsample        self.data = base_dist.sample([n_sample]).to(device).view(-1, self.batch_size, latent_dim)        self.target_sample = target_dist.sample([n_sample]).to(device).view(-1, self.batch_size, output_dim)        if self.max_val is None:            # Set the normalisation based on the first epoch of data            self.max_val = self.data.max()            self.min_val = self.data.min()        self.data = self.data        if self.normalise:            self.data = (self.data - self.min_val) / (self.max_val - self.min_val)    def get_loss(self, i, epoch, record=False, dpass=None):        # On the start of each epoch generate new samples, and then for each proceeding epoch iterate through the data        if i == 0:            self.update_data(record)        if dpass is None:            data = self.data[i]        else:            data = self.data[i]        if args.model in ['nsf', 'ni_nsf', 'affine']:            encoding, detJ = model(data, 1)        else:            encoding = model(data)        if args.beta_j > 0 and ((args.model == 'MLP') or (args.auto_J)):            detJ = get_log_det(data, model)        if args.det_only:            loss = - detJ.mean()        else:            sample = self.target_sample[i]            if args.true_likelihood:                encoding_distance = -prior.log_prob(encoding).mean()            else:                encoding_distance = dist_measure(encoding, sample)            if epoch >= args.train_ae:                loss = encoding_distance            else:                reconstruction = decoder(encoding)                loss = nn.L1Loss()(self.data[i], reconstruction) + encoding_distance * args.beta_dist            if args.beta_j:                loss -= detJ.mean() * args.beta_j        return lossloss_obj = data_handler(batch_size, nsteps_train, nsteps_val, normalise=args.normalise)if ld:    model.load_state_dict(torch.load(sv_dir + 'model_{}'.format(exp_name), map_location=device))if (not ld) or retrain:    for epoch in tbar:        if epoch == args.train_ae:            # Plot the encoding at initialisation of the distribution matching training on new data.            with torch.no_grad():                output = torch_utils.batch_predict(model, loss_obj.data)            pred_gauss = output.detach().cpu().numpy()            fig, ax = plt.subplots(1, 1, figsize=(5, 5))            plot2Dhist(pred_gauss, ax, 100)            fig.tight_layout()            fig.savefig(sv_dir + '/initial_dist_{}.png'.format(exp_name))            plt.clf()            left = args.std == 0.01            if args.base_dist == 'stars':                ticks = [-0.5, 0, 0.5]                lim = 0.75            if args.base_dist == 'shells':                ticks = [-1, 0, 1]                lim = 1.5            plot_density(pred_gauss, sv_dir + f'/initial_{exp_name}.png', left=left, bottom=True,                         ticks=ticks, lim=lim)        # Training        running_loss = []        for i in range(loss_obj.nsteps_train):            optimizer.zero_grad()            loss = loss_obj.get_loss(i, epoch)            loss.backward(retain_graph=True)            if args.gclip > 0:                clip_grad_value_(model.parameters(), args.gclip)            optimizer.step()            if scheduler is not None:                scheduler.step()            if i % monitor_interval == 0:                losses = loss_obj.get_loss(i, epoch)                running_loss += [loss.item()]                s = '[{}, {}] {}'.format(epoch + 1, i + 1, running_loss[-1])                # trec.set_description_str(s)        if (epoch == args.reset_optimizer) and (epoch != 0):            # Reset optimizer after training for args.reset_optimizer steps            optimizer, scheduler, reduce_lr_outer = get_optimizer_schedulers(model, n_epochs - args.reset_optimizer,                                                                             args.reset_lr, args.reset_wd, args.reset_p)        if (epoch == args.train_ae) and (args.reset_optimizer == 0):            # Reset the optimiser post training of the autoencoder            optimizer, scheduler, reduce_lr_outer = get_optimizer_schedulers(model, n_epochs - args.train_ae,                                                                             args.lr, args.wd, args.p)        # Update training loss trackers        train_save += [np.mean(running_loss, 0)]        # Validation        val_loss = np.zeros((loss_obj.nsteps_val))        for i in range(loss_obj.nsteps_val):            val_loss[i] = loss_obj.get_loss(i, epoch, record=True).item()        val_save += [np.mean(val_loss, 0)]        if args.scheduler == 'plateau':            reduce_lr_outer.step(np.mean(val_loss, 0)[0])    torch.save(model.state_dict(), sv_dir + '/model_{}'.format(exp_name))### Evaluatemodel.eval()nsamples = int(1e5)nbatch = 10data = data_handler(int(nsamples / nbatch), nbatch, normalise=args.normalise).dataif args.stochastic:    fig, ax = plt.subplots(1, 1, figsize=(5, 5))    output = torch_utils.batch_predict(model, data, encode=True)    mu = output[:, :2].detach().cpu().numpy()    plot2Dhist(mu, ax, 50)    ax.tick_params(axis='both', which='both', bottom=False, top=False, left=False, right=False, labelbottom=False,                   labelleft=False)    lim = 3.5    ax.set_xlim([-lim, lim])    ax.set_ylim([-lim, lim])    fig.tight_layout()    fig.savefig(sv_dir + '/mus_{}.png'.format(exp_name))if args.final_plot_encoding:    get_samples_model(model, base_dist, exp_name, args.base_dist, device)### Plottingif args.plt:    fig, ax = plt.subplots(1, 1, figsize=(20, 5))    ax.plot(train_save, label='Train')    ax.plot(val_save, '--', label='validation')    ax.set_title(args.dist_measure)    ax.legend()    ax.set_ylabel("loss")    ax.set_xlabel("epoch")    fig.savefig(sv_dir + '/training_{}.png'.format(exp_name))    with torch.no_grad():        output = torch_utils.batch_predict(model, data)    pred_gauss = output.detach().cpu().numpy()    fig, ax = plt.subplots(1, 1, figsize=(5, 5))    plot2Dhist(pred_gauss, ax, 100)    ax.tick_params(axis='both', which='both', bottom=False, top=False, left=False, right=False, labelbottom=False,                   labelleft=False)    lim = 3.5    ax.set_xlim([-lim, lim])    ax.set_ylim([-lim, lim])    fig.tight_layout()    fig.savefig(sv_dir + '/encoded_distribution_{}.png'.format(exp_name))    # The same but with ticks and no limits    fig, ax = plt.subplots(1, 1, figsize=(5, 5))    plot2Dhist(pred_gauss, ax, 100)    fig.tight_layout()    fig.savefig(sv_dir + '/encoded_distribution_ticks_{}.png'.format(exp_name))    bottom = args.base_dist == 'diamond'    plot_density(pred_gauss, sv_dir + '/encoded_paper_{}.png'.format(exp_name), left=False, bottom=bottom)    getCrossFeaturePlot(pred_gauss, f'{exp_name}_{latent_dim}', sv_dir)    if latent_dim == 2:        uniform_sample = torch.distributions.uniform.Uniform(torch.zeros(latent_dim).to(device) - 4.,                                                             torch.ones(latent_dim).to(device) * 4.,                                                             validate_args=None).sample([data.shape[0] * data.shape[1]])        filler = uniform_sample.view(*data.shape)        with torch.no_grad():            target = torch_utils.batch_predict(model, data)            target_fill = torch_utils.batch_predict(model, filler)        # Plot colored encodings of the different models so that the mapping of the mass can be visualised.        fig, ax_ = plt.subplots(1, 2, figsize=(10, 5))        ax = fig.axes        inp = data.view(-1, args.latent_dim)        plot_coloured(inp, inp, ax[0], 'Base distribution', args.base_dist, set_limits=False,                      filler=uniform_sample.cpu().numpy())        plot_coloured(inp, target, ax[1], 'Prediction', args.base_dist, set_limits=False, filler=target_fill)        fig.tight_layout()        fig.savefig(sv_dir + '/coloured_{}.png'.format(exp_name))        # Plot saliency maps in both the input and output spaces.        uniform_sample = torch.distributions.uniform.Uniform(torch.zeros(latent_dim).to(device) - 4.,                                                             torch.ones(latent_dim).to(device) * 4.,                                                             validate_args=None).sample([int(1e5)])        with torch.no_grad():            target = model(uniform_sample).cpu().numpy()        plot_density(target, sv_dir + '/encoded_uniform_{}.png'.format(exp_name))        from dmatch.data.plane import RotatedCheckerboardDataset, CheckerboardDataset        uniform_sample = RotatedCheckerboardDataset(int(1e5), flip_axes=True).data.to(device)        with torch.no_grad():            target = model(uniform_sample).cpu().numpy()        plot_density(target, sv_dir + '/encoded_anti_unif_{}.png'.format(exp_name))        # Saliency map        nsamp = int(1e5) if on_cluster() else int(1e4)        losses = [dist_measure]        names = [args.dist_measure]        additional_loss = [None]        beta = [0]        # If training the NSF take the derivative wrt the full loss and each of the components        if args.model in ['nsf', 'ni_nsf']:            def get_det(data, *args):                return model(data, get_det=1)[1].mean()            losses += [dist_measure] * 2            names += ['DetJ', 'Total_Loss']            additional_loss += [get_det] * 2            beta += [0, args.beta_j]        for loss_term, name, add_loss, beta in zip(losses, names, additional_loss, beta):            r_checkers = RotatedCheckerboardDataset(nsamp, flip_axes=True).data.to(device)            ood, r_y, ood_salience = get_salience(r_checkers, 'rotated_checkers', model, nsamp, batch_size, target_dist,                                                  device, dist_measure, sv_dir, f'{exp_name}_{name}', add_loss, beta)            checkers = CheckerboardDataset(nsamp, flip_axes=True).data.to(device)            ind, y, ind_salience = get_salience(checkers, 'checkers', model, nsamp, batch_size, target_dist, device,                                                dist_measure, sv_dir, f'{exp_name}_{name}', add_loss, beta)            x = torch.cat((ood, ind))            y = torch.cat((r_y, y))            salience = torch.cat((ood_salience, ind_salience))[:, 0]            plot_salience(x, y, salience, sv_dir + f'salience_joined_{exp_name}_{name}.png', n_bins=200)# With this you sample (n_calc * n_test) number of samples and calculate the kl divergence, This is repeated nrun timesif get_kl:    nrun = args.nrun    n_calc = args.ncalc    n_test = args.n_test    kl_div_info = get_kl_and_error(base_dist, target_dist, model, n_calc, nrun, n_test, device, g_chi=1)    print('Estimated KL divergence of {} with variance {}'.format(kl_div_info[0], kl_div_info[1]))    print('Estimated chi squared of {} with p value {}'.format(kl_div_info[2], kl_div_info[3]))    with open(sv_dir + '/score_{}.npy'.format(exp_name), 'wb') as f:        np.save(f, kl_div_info)    nbatch = 10    bound = 4    nbins = 50    nsamples = args.ncalc    data_generator = lambda nsamples: data_handler(int(nsamples / nbatch), nbatch).data    percent_ood, percent_oob, counts = get_ood(model, nsamples, nrun, bound, nbins, data_generator,                                               max_it=100)    with open(sv_dir + '/ood_{}.npy'.format(exp_name), 'wb') as f:        np.save(f, percent_ood)        np.save(f, percent_oob)if args.get_sinkhorn:    args.nrun = 10000    dist_measure = get_measure('sinkhorn')    btest = 1000    data_obj = data_handler(btest, args.nrun)    sv = np.empty(args.nrun)    for i in range(args.nrun):        normal_samples = target_dist.sample([btest])        with torch.no_grad():            encoded_samples = model(data_obj.data[i])        sv[i] = dist_measure(normal_samples.to(device), encoded_samples).item()    print(f'Sinkhorn Distance {np.mean(sv):.6f}')    print(args.base_dist)    with open(sv_dir + '/sinkhorn_{}.npy'.format(exp_name), 'wb') as f:        np.save(f, sv)    cum_mean = np.cumsum(sv) / np.arange(1, args.nrun + 1)    plt.figure()    plt.plot(cum_mean[10:])    plt.savefig(sv_dir + f'/mns_plot{str(latent_dim) + exp_name}.png')